import PyPDF2
from sentence_transformers import SentenceTransformer
import faiss
from transformers import pipeline
import numpy as np



def extract_text_from_pdf(pdf_path):
    """
    Cette fonction permet d'extraire le texte d'un fichier PDF page par page.
    
    Args:
    pdf_path : Le chemin du fichier PDF.
    
    Return:
    Liste contenant le texte de chaque page.
    """
    text = []
    with open(pdf_path, "rb") as f:
        reader = PyPDF2.PdfReader(f)
        for page in reader.pages:
            text.append(page.extract_text() or "")  # Ajoute un texte vide si None
    return text

def chunk_text(text_list, chunk_size=400):
    """
    Cette fonction permet de d√©couper le texte en segments (chunks) de taille d√©finie.
    
    Args:
    text_list: Liste de textes extraits du PDF (depuis "extract_text_from_pdf")
    chunk_size: Nombre de caract√®res par segment (chunk).

    Return:
    La liste des segments de texte (chunks).
    """
    chunks = []
    for text in text_list:
        for i in range(0, len(text), chunk_size):
            chunks.append(text[i:i+chunk_size])
    return chunks



def compute_embeddings(chunks):
    """
    Cette fonction permet de calculer les embeddings pour chaque chunk de texte.

    Args:
    chunks: Liste des segments de texte.
    model_name: Mod√®le SentenceTransformers utilis√©.

    Return:
    La liste des Embeddings (sous forme de numpy array).
    """
    model_name = "sentence-transformers/all-MiniLM-L6-v2"
    try:
        model = SentenceTransformer(model_name)
    except Exception as e:
        print(f"‚ö†Ô∏è HuggingFace model load failed: {e}")
        print("üëâ Trying to load local backup model...")
        model = SentenceTransformer("./models/miniLM")  

    embeddings = model.encode(chunks, convert_to_tensor=False)
    return np.array(embeddings)

#NB : Le mod√®le "all-MiniLM-L6-v2" g√©n√®re des embeddings de taille 384 (chaque chunk est cod√© par un vecteur de 384 nombres flottants)

def create_faiss_index(embeddings):
    """
    Cette fonction permet de cr√©er un index FAISS pour la recherche par similarit√© entre les embeddings.
    L'index est bas√© sur la distance L2 (a.k.a. Euclidean distance)
    
    Args:
    embeddings: Embeddings des chunks.

    Return:
    Index FAISS pr√™t √† l'emploi par la suite.
    """

    #V√©rification des embeddings
    if embeddings.shape[0] == 0:
        raise ValueError("Les embeddings sont vides ! V√©rifiez que le texte est bien extrait.")
    
    # Dimension des embeddings (384 pour ce mod√®le "MiniLM")
    d = embeddings.shape[1]  
    index = faiss.IndexFlatL2(d)
    index.add(embeddings)
    return index

def answer_question(question, chunks, index, model_name="bert-large-uncased-whole-word-masking-finetuned-squad"):
  
    model = SentenceTransformer("all-MiniLM-L6-v2")
    question_embedding = model.encode([question], convert_to_tensor=False)

    # Debug
    print(f"Question Embedding Shape: {question_embedding.shape}")
    print(f"FAISS Index Shape: {index.ntotal} vectors of dimension {index.d}")

    # FAISS search
    question_embedding = np.array(question_embedding).reshape(1, -1)
    distances, closest_chunk_idx = index.search(question_embedding, k=1)

    closest_chunk = chunks[closest_chunk_idx[0][0]]
    print(f" Le chunk le plus important: {closest_chunk}")

    # Load Q&A model
    qa_pipeline = pipeline("question-answering", model=model_name)

    #R√©ponse g√©n√©r√©e
    result = qa_pipeline(question=question, context=closest_chunk)

    # V√©rifier si la r√©ponse est vide ou trop courte
    if result["answer"].strip() == "" or len(result["answer"]) < 5:
        print("R√©ponse peu fiable, essayez de reformuler la question.")
        return "R√©ponse non trouv√©e. Essayez de reformuler votre question."

    print(f"R√©ponse g√©n√©r√©e: {result}")
    return result["answer"]

